{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 全部模型比较\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置通用参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization, Activation\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model\n",
    "import os\n",
    "importlib.reload(sys)\n",
    "root_path = os.path.split(sys.path[0])[0]\n",
    "# 词向量模型\n",
    "VECTOR_DIR = root_path+'/sgns/baike.vectors.bin'\n",
    "# 词向量模型的维度\n",
    "EMBEDDING_DIM = 200\n",
    "# 截取文本最大长度\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "# 验证集与测试集在总文本的比例\n",
    "VALIDATION_SPLIT = 0.40\n",
    "# 测试集的个数\n",
    "TEST_NUM = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 获取事件类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_texts = open(root_path+'/data/incident_rumor/virus_test.txt', encoding='utf-8').read().split('\\n')\n",
    "test_labels = open(root_path+'/data/incident_rumor/label_virus_test.txt', encoding='utf-8').read().split('\\n')\n",
    "train_texts = open(root_path+'/data/incident_rumor/news&incident_rumor.txt', encoding='utf-8').read().split('\\n')\n",
    "train_labels = open(root_path+'/data/incident_rumor/label_incident_rumor.txt', encoding='utf-8').read().split('\\n')\n",
    "# np.random.seed(100)\n",
    "# np.random.shuffle(train_texts)\n",
    "# np.random.seed(100)\n",
    "# np.random.shuffle(train_labels)\n",
    "all_labels = train_labels + test_labels\n",
    "all_texts = train_texts + test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 获取precision recall与f1对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fn\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fn\n"
     ]
    }
   ],
   "source": [
    "import keras_metrics as km\n",
    "precision = km.categorical_precision(label=1)\n",
    "# Calculate recall for the first label.\n",
    "recall = km.categorical_recall(label=1)\n",
    "f1 = km.categorical_f1_score(label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 读取词向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(VECTOR_DIR, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LSTM+word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30798 unique tokens.\n",
      "Shape of data tensor: (4642, 150)\n",
      "Shape of label tensor: (4642, 2)\n",
      "(3) split data set...\n",
      "train docs: 2785\n",
      "val docs: 957\n",
      "test docs: 900\n",
      "(4) load word2vec as embedding...\n",
      "7018 words not in w2v model\n",
      "(5) training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, dropout=0.5, recurrent_dropout=0.2, kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 200)          6159800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 6,628,282\n",
      "Trainable params: 468,482\n",
      "Non-trainable params: 6,159,800\n",
      "_________________________________________________________________\n",
      "['loss', 'acc', 'precision', 'recall', 'f1_score']\n",
      "Train on 2785 samples, validate on 957 samples\n",
      "Epoch 1/12\n",
      "2785/2785 [==============================] - 4s 1ms/step - loss: 0.6813 - acc: 0.8302 - precision: 0.7812 - recall: 0.8263 - f1_score: 0.8029 - val_loss: 0.4638 - val_acc: 0.9540 - val_precision: 1.0000 - val_recall: 0.9455 - val_f1_score: 0.9720\n",
      "Epoch 2/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.4090 - acc: 0.9573 - precision: 0.9778 - recall: 0.9527 - f1_score: 0.9650 - val_loss: 0.4288 - val_acc: 0.9519 - val_precision: 1.0000 - val_recall: 0.9492 - val_f1_score: 0.9739\n",
      "Epoch 3/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.3483 - acc: 0.9652 - precision: 0.9826 - recall: 0.9643 - f1_score: 0.9734 - val_loss: 0.3651 - val_acc: 0.9603 - val_precision: 1.0000 - val_recall: 0.9553 - val_f1_score: 0.9771\n",
      "Epoch 4/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.3309 - acc: 0.9720 - precision: 0.9911 - recall: 0.9690 - f1_score: 0.9799 - val_loss: 0.2948 - val_acc: 0.9770 - val_precision: 1.0000 - val_recall: 0.9733 - val_f1_score: 0.9865\n",
      "Epoch 5/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.4059 - acc: 0.9250 - precision: 0.9642 - recall: 0.9739 - f1_score: 0.9690 - val_loss: 0.4289 - val_acc: 0.9143 - val_precision: 1.0000 - val_recall: 0.9016 - val_f1_score: 0.9482\n",
      "Epoch 6/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.3216 - acc: 0.9465 - precision: 0.9649 - recall: 0.9497 - f1_score: 0.9572 - val_loss: 0.2366 - val_acc: 0.9864 - val_precision: 1.0000 - val_recall: 0.9866 - val_f1_score: 0.9933\n",
      "Epoch 7/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.2654 - acc: 0.9673 - precision: 0.9725 - recall: 0.9769 - f1_score: 0.9747 - val_loss: 0.2452 - val_acc: 0.9760 - val_precision: 1.0000 - val_recall: 0.9748 - val_f1_score: 0.9873\n",
      "Epoch 8/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.2233 - acc: 0.9759 - precision: 0.9877 - recall: 0.9794 - f1_score: 0.9835 - val_loss: 0.2013 - val_acc: 0.9864 - val_precision: 1.0000 - val_recall: 0.9827 - val_f1_score: 0.9913\n",
      "Epoch 9/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.3880 - acc: 0.9217 - precision: 0.9468 - recall: 0.9570 - f1_score: 0.9518 - val_loss: 0.3035 - val_acc: 0.9373 - val_precision: 1.0000 - val_recall: 0.9306 - val_f1_score: 0.9640\n",
      "Epoch 10/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.2875 - acc: 0.9447 - precision: 0.9520 - recall: 0.9484 - f1_score: 0.9502 - val_loss: 0.2239 - val_acc: 0.9613 - val_precision: 1.0000 - val_recall: 0.9560 - val_f1_score: 0.9775\n",
      "Epoch 11/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.2145 - acc: 0.9698 - precision: 0.9829 - recall: 0.9641 - f1_score: 0.9734 - val_loss: 0.1946 - val_acc: 0.9791 - val_precision: 1.0000 - val_recall: 0.9749 - val_f1_score: 0.9873\n",
      "Epoch 12/12\n",
      "2785/2785 [==============================] - 3s 1ms/step - loss: 0.1958 - acc: 0.9706 - precision: 0.9841 - recall: 0.9733 - f1_score: 0.9787 - val_loss: 0.1734 - val_acc: 0.9843 - val_precision: 1.0000 - val_recall: 0.9840 - val_f1_score: 0.9919\n",
      "(6) testing model...\n",
      "900/900 [==============================] - 1s 792us/step\n",
      "[0.33345689906014336, 0.9200000166893005, 0.9925992488861084, 0.8354689478874207, 0.8949374556541443]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(all_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "print ('(3) split data set...')\n",
    "p1 = int(len(data)*(1 - VALIDATION_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = labels[:p1]\n",
    "x_val = data[p1:len(data)-TEST_NUM]\n",
    "y_val = labels[p1:len(labels)-TEST_NUM]\n",
    "x_test = data[len(data)-TEST_NUM:]\n",
    "y_test = labels[len(labels)-TEST_NUM:]\n",
    "print ('train docs: '+str(len(x_train)))\n",
    "print ('val docs: '+str(len(x_val)))\n",
    "print ('test docs: '+str(len(x_test)))\n",
    "\n",
    "print ('(4) load word2vec as embedding...')\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "not_in_model = 0\n",
    "in_model = 0\n",
    "for word, i in word_index.items():\n",
    "    if str(word) in w2v_model:\n",
    "        in_model += 1\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[str(word)], dtype='float32')\n",
    "    else:\n",
    "        not_in_model += 1\n",
    "print (str(not_in_model)+' words not in w2v model')\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print ('(5) training model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(256, dropout=0.5, recurrent_dropout=0.2,W_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png',show_shapes=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc',precision, recall,f1])\n",
    "print (model.metrics_names)\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),validation_split=0.33, epochs=12, batch_size=128,shuffle=True)\n",
    "print ('(6) testing model...')\n",
    "print (model.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}